import tokenize from 'json-lexer';

const identity = inp => inp;

export function colorizeJson(input, chalk) {
  const formatters = {
    punctuator: chalk.white,
    key: chalk.white,
    string: chalk.green,
    number: chalk.yellow,
    literal: chalk.bold,
    whitespace: identity
  };
  const json = JSON.stringify(input, null, 2);
  return tokenize(json).map((token, i, arr) => {
    // Note how the following only works because we pretty-print the JSON
    const prevToken = i === 0 ? token : arr[i - 1];

    if (token.type === 'string' && prevToken.type === 'whitespace' && /^\n\s+$/.test(prevToken.value)) {
      return { ...token,
        type: 'key'
      };
    }

    return token;
  }).map(token => {
    const formatter = formatters[token.type] || identity;
    return formatter(token.raw);
  }).join('');
}